---
title: "随机森林"
author: "赵奇"
date: "2015年11月6日"
output: 
  pdf_document: 
    keep_tex: yes
    latex_engine: xelatex
---

随机森林也是一种集成学习算法，使用CART训练基分类器，因此是树型分基类器$\left\{ {h\left( {x,{\beta _k}} \right),k = 1,2, \ldots ,n} \right\}$的集合。其中基分类器$h\left( {x,{\beta _k}} \right)$是充分生长、没有剪枝的树。  

CART是一种高效的决策树算法。选择最佳分裂节点，有多种准则可选，例如Gini系数、Entropy系数和Classification Error等。选择最佳划分度量通常是根据划分后子女结点不纯度的程度。不纯的程度越低，类分布就越倾斜。例如，类分布为（0,1）的结点具有零不纯性，而均衡分布（0.5,0.5）的结点具有最高的不纯性。设 表示给定结点 中属于类 的记录所占的比例，不纯性度量的例子包括：  

$${\rm{Entropy}}\left( t \right) =  - \sum\limits_{i = 0}^{c - 1} {p\left( {i|t} \right){{\log }_2}p\left( {i|t} \right)} $$

$${\rm{Gini}}\left( t \right) = 1 - \sum\limits_{i = 0}^{c - 1} {\left( {p{{\left( {i|t} \right)}^2}} \right)}$$  

$${\rm{Classification error}}\left( t \right) = 1 - \mathop {\max }\limits_i \left[ {p\left( {i|t} \right)} \right]$$

其中$c$是类的个数，并且在计算熵时$O{\log _2}0 = 0$。  

CART算法根据结点分裂过程中Gini系数最小化原则，采用二元递归划分方法，将当前结点上的样本划分为两个样本集，直到达到某个预订的停止标准。因为是二元划分，因此每个非叶子结点都有两个分支。对于分类任务，采用简单多数投票法的结果作为随机森林的输出；对于回归任务，采用简单算术平均作为随机森林的输出。随机森林算法的步骤如下：  

+ 假设要构建规模为$k$的随机森林，即森林中包含$k$颗决策树，使用Bagging算法产生个$k$自助训练数据集；
+ 每一颗决策树对应一个自助训练数据集进行训练。假设样本的特征数为$M$，单颗决策树的生长过程如下：在树的每个分裂节点处随机挑选 $mtry$个特征作为候选特征，按照CART算法节点分裂的准则从$mtry$个候选特征中挑选出最优的特征进行分裂，每一个树都是充分生长的，没有进行过剪枝的。

+ 使用训练好的由颗决策树组成的分类器来预测新的数据，对于分类任务，使用简单多数投票法得到结果；对于回归任务，使用简单算术平均法得到结果。  


因为使用了Bootstrap抽样方法法，因此每次抽样生成一份自助样本集时，有接近37%的样本不会出现相应的自助样本集中，通常将这些剩余的样本数据成为袋外数据（Out-Of-Bag，OOB）。袋外数据可以用来估计随机森林的泛化误差以及评估特征重要性。Breiman通过实验证明，OBB误分率是随机森林泛化误差的无偏估计，即使用袋外数据和除训练集以外的测试集对泛化误差估计的效果是一样的。大量实验证明随机森林分类器具有非常好的性能，在Kaggle之类的竞赛中也得到了非常广泛的应用。

